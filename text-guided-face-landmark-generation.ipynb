{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T08:19:29.109730Z","iopub.status.busy":"2023-10-16T08:19:29.109075Z","iopub.status.idle":"2023-10-16T08:19:51.480416Z","shell.execute_reply":"2023-10-16T08:19:51.479169Z","shell.execute_reply.started":"2023-10-16T08:19:29.109695Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["mkdir: cannot create directory ‘text2landmark_68_2d’: File exists\n","/kaggle/working/text2landmark_68_2d\n","Downloading...\n","From: https://drive.google.com/uc?id=1-2phy4gND3k1F7Wi-HB4qzGs5ImKztrp\n","To: /kaggle/working/text2landmark_68_2d/text2landmark_68_2d.zip\n","100%|██████████████████████████████████████| 9.74M/9.74M [00:00<00:00, 68.2MB/s]\n","Archive:  text2landmark_68_2d.zip\n","replace landmarks.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n","/kaggle/working\n"]}],"source":["!mkdir text2landmark_68_2d\n","%cd text2landmark_68_2d\n","!gdown 1-2phy4gND3k1F7Wi-HB4qzGs5ImKztrp\n","!unzip text2landmark_68_2d.zip\n","!rm text2landmark_68_2d.zip\n","%cd .."]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T14:16:15.731237Z","iopub.status.busy":"2023-10-16T14:16:15.730868Z","iopub.status.idle":"2023-10-16T14:16:30.225257Z","shell.execute_reply":"2023-10-16T14:16:30.223861Z","shell.execute_reply.started":"2023-10-16T14:16:15.731208Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting git+https://github.com/openai/CLIP.git\n","  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-g1qk1z4k\n","  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-g1qk1z4k\n","  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n","  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: ftfy in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (6.1.1)\n","Requirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (2023.6.3)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (4.66.1)\n","Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (2.0.0)\n","Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (0.15.1)\n","Requirement already satisfied: wcwidth>=0.2.5 in /opt/conda/lib/python3.10/site-packages (from ftfy->clip==1.0) (0.2.6)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.12.2)\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (4.6.3)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (1.12)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.1.2)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (1.23.5)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (2.31.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (9.5.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->clip==1.0) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (3.1.0)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (2023.7.22)\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->clip==1.0) (1.3.0)\n"]}],"source":["!pip install git+https://github.com/openai/CLIP.git\n","# !pip install transformers"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T14:16:30.228629Z","iopub.status.busy":"2023-10-16T14:16:30.228222Z","iopub.status.idle":"2023-10-16T14:16:30.236263Z","shell.execute_reply":"2023-10-16T14:16:30.235207Z","shell.execute_reply.started":"2023-10-16T14:16:30.228589Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.optim import Adam\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from torch.autograd import Variable\n","import clip\n","# from transformers import CLIPTokenizer, CLIPTextModel\n","\n","import os\n","import math\n","import time\n","import pickle\n","import random\n","import collections\n","import numpy as np\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T14:16:30.239060Z","iopub.status.busy":"2023-10-16T14:16:30.238421Z","iopub.status.idle":"2023-10-16T14:16:30.260015Z","shell.execute_reply":"2023-10-16T14:16:30.258898Z","shell.execute_reply.started":"2023-10-16T14:16:30.239003Z"},"trusted":true},"outputs":[],"source":["class CelebALandmark(Dataset):\n","    def __init__(self, data_path, size=1024, split=0):\n","        \"\"\"\n","        split: 0 train, 1 testing, 2 validation\n","        \"\"\"\n","        self.size = size\n","        self.landmarks_path = os.path.join(data_path, 'landmarks.txt')\n","        self.captions_path = os.path.join(data_path, 'captions.pickle')\n","        self.splits_path = os.path.join(data_path, 'splits')\n","        self.split = split\n","        self.names = []\n","        self.landmarks = []\n","        self.captions = []\n","\n","        if split == 0:\n","            self.filenames = pickle.load(open(f\"{self.splits_path}/train_filenames.pickle\", \"rb\"))\n","        elif split == 1:\n","            self.filenames = pickle.load(open(f\"{self.splits_path}/test_filenames.pickle\", \"rb\"))\n","        self.all_captions = pickle.load(open(self.captions_path, \"rb\"))\n","\n","        all_labels = open(self.landmarks_path, 'r').readlines()\n","        for label in all_labels:\n","            name, shape, _ = self.parse_label(label)\n","            filename = name.split('.')[0]\n","            if filename in self.filenames:\n","                self.names.append(name)\n","                shape = np.array(shape) // (self.size // 64) # convert to 64\n","#                 shape = shape[:36]\n","                heatmaps = self.keypoints2heatmaps(shape, size=(64, 64))\n","                self.landmarks.append(heatmaps)\n","                captions = self.all_captions[f'{filename}.txt']\n","                self.captions.append(captions)\n","\n","    def parse_label(self, label):\n","        l = label.strip().split()\n","        name = l[0]\n","        shape = []\n","        ori_gaze = []\n","        if len(l) > 107:\n","            w_ori, h_ori = [int(_) for _ in l[1].split('-')]\n","            for l_ in l[2:108]:\n","                w, h = [int(_) for _ in l_.split('-')]\n","                shape.append([w, h])\n","            for l_ in l[108:]:\n","                ori_gaze.append(int(l_))\n","        else:\n","            for l_ in l[1:]:\n","                l_s = l_.split('-')\n","                if l_.startswith('-'):\n","                    w, h = -int(l_s[1]), int(l_s[2])\n","                elif l_s[1] == '':\n","                    w, h = int(l_s[0]), -int(l_s[2])\n","                else:\n","                    w, h = int(l_s[0]), int(l_s[1])\n","                h = self.size - h # flip landmarks due to problem in saving\n","                shape.append([w, h])\n","\n","        return name, shape, ori_gaze\n","\n","    def keypoints2heatmaps(self, keypoints, size=(64, 64)):\n","        keypoints = keypoints.astype(int)\n","        heatmaps = np.zeros(size + (keypoints.shape[0],), dtype=np.int8)\n","        for k in range(keypoints.shape[0]):\n","            x, y = keypoints[k]\n","            x, y = min(x, size[0] - 1), min(y, size[0] - 1)\n","            if x < 0 or y < 0:\n","                continue\n","            heatmaps[y, x, k] = 1\n","        return heatmaps\n","\n","    def __len__(self):\n","        return len(self.landmarks)\n","\n","    def __getitem__(self, item):\n","        landmark = self.landmarks[item]\n","        caption = random.choice(self.captions[item])\n","        caption_1 = random.choice(random.choice(self.captions))\n","        caption_2 = random.choice(random.choice(self.captions))\n","\n","        return landmark, caption, caption_1, caption_2"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T14:16:30.263831Z","iopub.status.busy":"2023-10-16T14:16:30.263151Z","iopub.status.idle":"2023-10-16T14:16:30.278692Z","shell.execute_reply":"2023-10-16T14:16:30.277712Z","shell.execute_reply.started":"2023-10-16T14:16:30.263742Z"},"trusted":true},"outputs":[],"source":["def init_networks(network, name, init_type='normal', init_gain=0.02, verbose=False):\n","        def init_params(m):\n","            classname = m.__class__.__name__\n","            if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n","                if init_type == 'normal':\n","                    nn.init.normal_(m.weight.data, 0.0, init_gain)\n","                elif init_type == 'orthogonal':\n","                    nn.init.orthogonal_(m.weight.data, init_gain)\n","                elif init_type == 'xavier_normal':\n","                    nn.init.xavier_normal_(m.weight.data, init_gain)\n","                elif init_type == 'kaiming_normal':\n","                    nn.init.kaiming_normal_(m.weight.data)\n","                else:\n","                    raise NotImplementedError(init_type)\n","                if hasattr(m, 'bias') and m.bias is not None:\n","                    nn.init.constant_(m.bias.data, 0.0)\n","            elif classname.find('BatchNorm2d') != -1:\n","                nn.init.normal_(m.weight.data, 1.0, init_gain)\n","                nn.init.constant_(m.bias.data, 0.0)\n","        \n","        network.apply(init_params)\n","        if verbose:\n","            print(f'[INFO] Network {name} initialized')"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T14:16:30.281231Z","iopub.status.busy":"2023-10-16T14:16:30.280473Z","iopub.status.idle":"2023-10-16T14:16:30.308063Z","shell.execute_reply":"2023-10-16T14:16:30.306896Z","shell.execute_reply.started":"2023-10-16T14:16:30.281192Z"},"trusted":true},"outputs":[],"source":["\"\"\"\n","Inspired from TIPS: Text-Induced Pose Synthesis Stage-1 network\n","\"\"\"\n","\n","\n","import torch\n","import torch.nn as nn\n","\n","\n","def linear(in_features, out_features, bias=True):\n","    return nn.Sequential(\n","        nn.Linear(in_features, out_features, bias=bias),\n","        nn.LeakyReLU(inplace=True)\n","    )\n","\n","\n","def upconv4x(in_channels, out_channels, bias=False):\n","    return nn.Sequential(\n","        nn.ConvTranspose2d(in_channels, out_channels, 4, 4, 0, bias=bias),\n","        nn.BatchNorm2d(out_channels),\n","        nn.ReLU(inplace=True)\n","    )\n","\n","\n","def upconv2x_hidden(in_channels, out_channels, bias=False):\n","    return nn.Sequential(\n","        nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1, bias=bias),\n","        nn.BatchNorm2d(out_channels),\n","        nn.ReLU(inplace=True)\n","    )\n","\n","\n","def upconv2x_output(in_channels, out_channels, bias=False):\n","    return nn.Sequential(\n","        nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1, bias=bias),\n","        nn.Tanh()\n","    )\n","\n","\n","def conv2x(in_channels, out_channels, bias=False):\n","    return nn.Sequential(\n","        nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1, bias=bias),\n","        nn.LeakyReLU(0.2, True)\n","    )\n","\n","\n","def conv1x(in_channels, out_channels, bias=False):\n","    return nn.Sequential(\n","        nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=bias),\n","        nn.LeakyReLU(0.2, True)\n","    )\n","\n","\n","def conv_output(in_channels, out_channels, bias=False):\n","    return nn.Sequential(\n","        nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=1, padding=0, bias=bias),\n","        # nn.Sigmoid()\n","    )\n","\n","\n","class TextEncoder(nn.Module):\n","    def __init__(self, device='cpu'):\n","        super(TextEncoder, self).__init__()\n","\n","        # Text Transformer\n","        self.clip, _ = clip.load('ViT-B/32', device)\n","        self.clip.initialize_parameters()\n","\n","    def encode_text(self, text, device):\n","        with torch.no_grad():\n","            text = clip.tokenize(text, truncate=True).to(device)\n","            x = self.clip.token_embedding(text).type(self.clip.dtype)  # [batch_size, n_ctx, d_model]\n","\n","            x = x + self.clip.positional_embedding.type(self.clip.dtype)\n","            x = x.permute(1, 0, 2)  # NLD -> LND\n","            x = self.clip.transformer(x)\n","            x = self.clip.ln_final(x).type(self.clip.dtype)\n","            # B, D\n","            x = x[-1]\n","\n","        return x\n","\n","    def forward(self, text, device):\n","        return self.encode_text(text, device)\n","\n","\n","class Text2LandmarkG(nn.Module):\n","    def __init__(self, noise_dim=128, heatmap_channels=68, ngf=32):\n","        super(Text2LandmarkG, self).__init__()\n","        self.combined_up1 = upconv4x(noise_dim*2, ngf*8)\n","        self.combined_up2 = upconv2x_hidden(ngf*8, ngf*4)\n","        self.combined_up3 = upconv2x_hidden(ngf*4, ngf*2)\n","        self.combined_up4 = upconv2x_hidden(ngf*2, ngf)\n","        self.combined_up5 = upconv2x_output(ngf, heatmap_channels)\n","\n","        self.text_linear = linear(512, noise_dim)\n","        # self.text_trans_encoder = nn.TransformerEncoder(textTransEncoderLayer,\n","        #                                                 num_layers=num_text_layers)\n","        # self.text_ln = nn.LayerNorm(text_latent_dim)\n","\n","        init_networks(self, 'Text2LandmarkG', verbose=True)\n","\n","\n","    def forward(self, x, text):\n","        x_noise = x.view(x.size(0), -1, 1, 1)\n","\n","        # B, D\n","        text_embed = self.text_linear(text)\n","        # text_embed = self.text_trans_encoder(text_embed)\n","        # text_embed = self.text_ln(text_embed)\n","        # B, D, 1, 1\n","        text_embed = text_embed.view(text_embed.size(0), -1, 1, 1)\n","\n","        combined = torch.cat((x_noise, text_embed), dim=1)\n","\n","        y = self.combined_up1(combined)\n","        y = self.combined_up2(y)\n","        y = self.combined_up3(y)\n","        y = self.combined_up4(y)\n","        y = self.combined_up5(y)\n","\n","        # B, 64, 64, D\n","        y = y.permute(0, 2, 3, 1)\n","\n","        return y\n","\n","\n","class Text2LandmarkD(nn.Module):\n","    def __init__(self, noise_dim=128, in_channels=68, ndf=32):\n","        super(Text2LandmarkD, self).__init__()\n","        self.conv1 = conv2x(in_channels, ndf)\n","        self.conv2 = conv2x(ndf, ndf * 2, bias=False)\n","        self.conv3 = conv2x(ndf * 2, ndf * 4, bias=False)\n","        self.conv4 = conv2x(ndf * 4, ndf * 4, bias=False)\n","        self.conv5 = conv1x(ndf * 8, ndf * 8, bias=False)\n","        self.conv6 = conv_output(ndf * 8, 1, bias=True)\n","\n","        self.text_linear = linear(512, noise_dim)\n","        # self.text_trans_encoder = nn.TransformerEncoder(textTransEncoderLayer,\n","        #                                                 num_layers=num_text_layers)\n","        # self.text_ln = nn.LayerNorm(text_latent_dim)\n","\n","        init_networks(self, 'Text2LandmarkD', verbose=True)\n","\n","    def forward(self, x, text):\n","        x = x.permute(0, 3, 1, 2)\n","        y = self.conv1(x)\n","        y = self.conv2(y)\n","        y = self.conv3(y)\n","        y = self.conv4(y)\n","\n","        # B, D\n","        text_embed = self.text_linear(text)\n","        # text_embed = self.text_trans_encoder(text_embed)\n","        # text_embed = self.text_ln(text_embed)\n","        # B, D, 16\n","        text_embed = text_embed.unsqueeze(2).repeat(1, 1, 16)\n","        # B, D, 4, 4\n","        text_embed = text_embed.view(text_embed.size(0), text_embed.size(1), 4, 4)\n","\n","        combined = torch.cat((y, text_embed), dim=1)\n","        y = self.conv5(combined)\n","        y = self.conv6(y)\n","\n","        return y"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T14:16:30.310407Z","iopub.status.busy":"2023-10-16T14:16:30.309738Z","iopub.status.idle":"2023-10-16T14:16:30.331923Z","shell.execute_reply":"2023-10-16T14:16:30.330760Z","shell.execute_reply.started":"2023-10-16T14:16:30.310371Z"},"trusted":true},"outputs":[],"source":["def save_state(model_G, model_D, optimizer_G, optimizer_D, epoch_no, best=False):\n","    params = {'optimizer_G': optimizer_G.state_dict(),\n","              'optimizer_D': optimizer_D.state_dict(),\n","              'epoch': epoch_no}\n","    save_postfix = '_best' if best else f'_last'\n","    if isinstance(model_G, nn.DataParallel):\n","        torch.save(model_G.module.state_dict(), save_path + f\"/checkpoints/model_G{save_postfix}.pth\")\n","        torch.save(model_D.module.state_dict(), save_path + f\"/checkpoints/model_D{save_postfix}.pth\")\n","    else:\n","        torch.save(model_G.state_dict(), save_path + f\"/checkpoints/model_G{save_postfix}.pth\")\n","        torch.save(model_D.state_dict(), save_path + f\"/checkpoints/model_D{save_postfix}.pth\")\n","    torch.save(params, save_path + f\"/checkpoints/params{save_postfix}.pth\")\n","\n","def heatmaps2keypoints(heatmaps, confidence):\n","    keypoints = []\n","    for k in range(heatmaps.shape[2]):\n","        heatmap_k = heatmaps[:, :, k]\n","        proba_max = np.max(heatmap_k)\n","        if proba_max > confidence:\n","            y, x = np.where(heatmap_k == proba_max)\n","            y, x = y[0], x[0]\n","        else:\n","            y, x = -1, -1\n","        keypoints.append((x, y))\n","    return np.int32(keypoints)\n","\n","def keypoints2heatmaps(keypoints, size=(64, 64)):\n","    keypoints = keypoints.reshape(-1, 2).astype(np.int32)\n","    heatmaps = np.zeros(size + (keypoints.shape[0],), dtype=np.float32)\n","    for k in range(keypoints.shape[0]):\n","        x, y = keypoints[k]\n","        if x < 0 or y < 0:\n","            continue\n","        heatmaps[y, x, k] = 1\n","    return heatmaps\n","\n","def visualize(fake_lmks, real_lmks, caption, epoch_no):\n","    fig = plt.figure(figsize=(16, 8))\n","#     fig.suptitle(caption)\n","    for i in range(16):\n","        kB = heatmaps2keypoints(fake_lmks[i], 0.2)\n","        fake_lmk = np.where(kB < 0, -1, kB * 4)\n","        kB = heatmaps2keypoints(real_lmks[i], 0.2)\n","        real_lmk = np.where(kB < 0, -1, kB * 4)\n","\n","        plot_style = dict(marker='o',\n","                        markersize=4,\n","                        linestyle='-',\n","                        lw=2)\n","\n","        pred_type = collections.namedtuple('prediction_type', ['slice', 'color'])\n","        pred_types = {'face': pred_type(slice(0, 17), (0.682, 0.780, 0.909, 0.5)),\n","                    'eyebrow1': pred_type(slice(17, 22), (1.0, 0.498, 0.055, 0.4)),\n","                    'eyebrow2': pred_type(slice(22, 27), (1.0, 0.498, 0.055, 0.4)),\n","                    'nose': pred_type(slice(27, 31), (0.345, 0.239, 0.443, 0.4)),\n","                    'nostril': pred_type(slice(31, 36), (0.345, 0.239, 0.443, 0.4)),\n","                    'eye1': pred_type(slice(36, 42), (0.596, 0.875, 0.541, 0.3)),\n","                    'eye2': pred_type(slice(42, 48), (0.596, 0.875, 0.541, 0.3)),\n","                    'lips': pred_type(slice(48, 60), (0.596, 0.875, 0.541, 0.3)),\n","                    'teeth': pred_type(slice(60, 68), (0.596, 0.875, 0.541, 0.4))\n","                    }\n","\n","        ax = fig.add_subplot(4, 8, 2 * i + 1)\n","        ax.title.set_text('Fake')\n","        for pred_type in pred_types.values():\n","            ax.plot(fake_lmk[pred_type.slice, 0],\n","                    fake_lmk[pred_type.slice, 1],\n","                    color=pred_type.color, **plot_style)\n","        ax.axis('off')\n","\n","        ax = fig.add_subplot(4, 8, 2 * i + 2)\n","        ax.title.set_text('Real')\n","        for pred_type in pred_types.values():\n","            ax.plot(real_lmk[pred_type.slice, 0],\n","                    real_lmk[pred_type.slice, 1],\n","                    color=pred_type.color, **plot_style)\n","        ax.axis('off')\n","\n","    plt.show()\n","    fig.savefig(f\"{save_path}/results/{epoch_no}.png\")\n","\n","def plot_loss(train_loss_epoch, valid_loss_epoch, train_loss, valid_loss, name='G'):\n","    fig, ax = plt.subplots(figsize=(6, 4))\n","    ax.plot(train_loss_epoch, train_loss, label=f'Train {name}')\n","    ax.plot(valid_loss_epoch, valid_loss, label=f'Valid {name}')\n","    ax.grid(True)\n","    plt.legend()\n","    plt.show()\n","    fig.savefig(f\"{save_path}/loss_{name}.png\")"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T14:16:30.334486Z","iopub.status.busy":"2023-10-16T14:16:30.333595Z","iopub.status.idle":"2023-10-16T14:16:30.347400Z","shell.execute_reply":"2023-10-16T14:16:30.346369Z","shell.execute_reply.started":"2023-10-16T14:16:30.334449Z"},"trusted":true},"outputs":[],"source":["batch_size_train = 64\n","batch_size_valid = 64\n","batch_size_test = 8\n","n_epoch = 50\n","valid_epoch_interval = 2\n","do_validation = False\n","start_epoch = 0\n","lambda_gp = 2\n","lr = 0.0001\n","\n","save_path = f'e50-lgp2-lr0_0001-128-68-64-l36-cr3'"]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T14:16:30.349967Z","iopub.status.busy":"2023-10-16T14:16:30.348942Z","iopub.status.idle":"2023-10-16T14:16:51.555156Z","shell.execute_reply":"2023-10-16T14:16:51.552866Z","shell.execute_reply.started":"2023-10-16T14:16:30.349931Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":[">>> Training dataset length: 23698\n"]}],"source":["data_path = '/kaggle/working/text2landmark_68_2d/'\n","\n","train_dataset = CelebALandmark(data_path, split=0)\n","print('>>> Training dataset length: {:d}'.format(train_dataset.__len__()))\n","train_loader = DataLoader(train_dataset, batch_size=batch_size_train, shuffle=True, num_workers=0, pin_memory=True)\n","\n","if do_validation:\n","    valid_dataset = CelebALandmark(data_path, split=1)\n","    print('>>> Validation dataset length: {:d}'.format(valid_dataset.__len__()))\n","    valid_loader = DataLoader(valid_dataset, batch_size=batch_size_valid, shuffle=True, num_workers=0, pin_memory=True)\n","\n","# test_dataset = CelebALandmark(data_path, split=2)\n","# print('>>> Test dataset length: {:d}'.format(test_dataset.__len__()))\n","# test_loader = DataLoader(test_dataset, batch_size=batch_size_test, shuffle=False, num_workers=0, pin_memory=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T12:40:16.072964Z","iopub.status.busy":"2023-10-16T12:40:16.072480Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print('Using device: %s' % device)\n","\n","if not os.path.exists(save_path):\n","    os.makedirs(save_path)\n","if not os.path.exists(f'{save_path}/results'):\n","    os.makedirs(f'{save_path}/results')\n","if not os.path.exists(f'{save_path}/checkpoints'):\n","    os.makedirs(f'{save_path}/checkpoints')\n","\n","model_G = Text2LandmarkG(128, 68, 32)\n","model_D = Text2LandmarkD(128, 68, 32)\n","text_encoder = TextEncoder(device)\n","\n","model_G.to(device)\n","model_D.to(device)\n","\n","criterion = nn.MSELoss()\n","\n","optimizer_G = Adam(model_G.parameters(), lr=lr, betas=(0, 0.9))\n","optimizer_D = Adam(model_D.parameters(), lr=lr, betas=(0, 0.9))\n","\n","train_loss_G = []\n","train_loss_D = []\n","valid_loss_G = []\n","valid_loss_D = []\n","train_loss_epoch = []\n","valid_loss_epoch = []\n","\n","for epoch_no in range(start_epoch, n_epoch):\n","    model_G.train()\n","    model_D.train()\n","    avg_loss_D = 0\n","    avg_loss_G = 0\n","    with tqdm(train_loader) as it:\n","        for batch_no, batch in enumerate(it, start=1):\n","            landmarks, captions, captions_1, captions_2 = batch\n","            b_size = len(landmarks)\n","            real_lmks = landmarks.to(device).type(torch.float)\n","\n","            ###### Text Encoder ######\n","            # encoded_captions = text_encoder(captions, device).type(torch.float)\n","            # encoded_captions_1 = text_encoder(captions_1, device).type(torch.float)\n","            # encoded_captions_2 = text_encoder(captions_2, device).type(torch.float)\n","            encoded_captions = torch.zeros(size=(b_size, 512)).to(device).type(torch.float)\n","            encoded_captions_1 = torch.zeros(size=(b_size, 512)).to(device).type(torch.float)\n","            encoded_captions_2 = torch.zeros(size=(b_size, 512)).to(device).type(torch.float)\n","\n","            ##### Discriminator ######\n","            # z = torch.normal(0, 1, size=(b_size, 128)).to(device)\n","            for _ in range(1):\n","                for p in model_D.parameters():\n","                    p.requires_grad = True\n","                for p in model_G.parameters():\n","                    p.requires_grad = False\n","                model_D.zero_grad()\n","                # real\n","                label = torch.full((b_size,), 1, dtype=torch.float, device=device)\n","                real_D = model_D(real_lmks, encoded_captions).view(-1)\n","                loss_real_D = criterion(real_D, label)\n","\n","                # fake\n","                z = torch.normal(0, 1, size=(b_size, 128)).to(device)\n","                fake_lmks = model_G(z, encoded_captions)\n","                label = torch.full((b_size,), 0, dtype=torch.float, device=device)\n","                fake_D = model_D(fake_lmks, encoded_captions).view(-1)\n","                loss_fake_D = criterion(fake_D, label)\n","\n","                # grad penalty\n","                alpha = torch.rand(b_size, 1, 1, 1).to(device)\n","                interp_lmks = alpha * real_lmks.data + (1- alpha) * fake_lmks.data\n","                interp_lmks = Variable(interp_lmks, requires_grad=True).to(device)\n","                interp_D = model_D(interp_lmks, encoded_captions)\n","                weight = torch.ones(interp_D.size()).to(device)\n","                gradients = torch.autograd.grad(outputs=interp_D,\n","                                                inputs=interp_lmks,\n","                                                grad_outputs=weight,\n","                                                retain_graph=True,\n","                                                create_graph=True,\n","                                                only_inputs=True)[0]\n","                gradients = gradients.view(gradients.size(0), -1)\n","                gradients_norm = torch.sqrt(torch.sum(gradients ** 2, dim=1) + 1e-12)\n","                loss_gp_D = ((gradients_norm - 1) ** 2).mean()\n","\n","                # discriminator loss\n","                loss_D = loss_real_D + loss_fake_D + loss_gp_D * lambda_gp\n","                # optimizer_D.zero_grad()\n","                loss_D.backward()\n","                optimizer_D.step()\n","            avg_loss_D += loss_D.item()\n","\n","            ####### Generator ########\n","            for _ in range(1):\n","                for p in model_D.parameters():\n","                    p.requires_grad = False\n","                for p in model_G.parameters():\n","                    p.requires_grad = True\n","                model_G.zero_grad()\n","                z = torch.normal(0, 1, size=(b_size, 128)).to(device)\n","                fake_lmks = model_G(z, encoded_captions)\n","                fake_D = model_D(fake_lmks, encoded_captions).view(-1)\n","                label = torch.full((b_size,), 1, dtype=torch.float, device=device)\n","                loss_fake_G = criterion(fake_D, label)\n","\n","                interp_captions = (encoded_captions_1 + encoded_captions_2) / 2\n","                z = torch.normal(0, 1, size=(b_size, 128)).to(device)\n","                fake_interp_lmks = model_G(z, interp_captions)\n","                fake_interp_D = model_D(fake_interp_lmks, interp_captions).view(-1)\n","                loss_interp_G = criterion(fake_interp_D, label)\n","\n","                loss_G = loss_fake_G + loss_interp_G\n","\n","                # optimizer_G.zero_grad()\n","                loss_G.backward()\n","                optimizer_G.step()\n","            avg_loss_G += loss_G.item()\n","\n","            it.set_postfix(\n","                ordered_dict={\n","                    \"loss_D\": avg_loss_D / batch_no,\n","                    \"loss_G\": avg_loss_G / batch_no,\n","                    \"epoch\": epoch_no,\n","                },\n","                refresh=False,\n","            )\n","\n","    # rand_ind = np.random.randint(b_size)\n","    fake_lmks = fake_lmks[:16].detach().cpu().numpy()\n","    real_lmks = real_lmks[:16].detach().cpu().numpy()\n","    caption = captions[:16]\n","    visualize(fake_lmks, real_lmks, caption, epoch_no)\n","\n","    train_loss_G.append(avg_loss_G / batch_no)\n","    train_loss_D.append(avg_loss_D / batch_no)\n","    train_loss_epoch.append(epoch_no)\n","\n","    if do_validation and (epoch_no + 1) % valid_epoch_interval == 0:\n","        model_G.eval()\n","        model_D.eval()\n","        avg_loss_D_valid = 0\n","        avg_loss_G_valid = 0\n","        best_valid_loss = 0\n","        with torch.no_grad():\n","            with tqdm(valid_loader) as it:\n","                for batch_no, batch in enumerate(it, start=1):\n","                    landmarks, captions, captions_1, captions_2 = batch\n","                    b_size = len(landmarks)\n","                    real_lmks = landmarks.to(device).type(torch.float)\n","\n","                    ###### Text Encoder ######\n","                    encoded_captions = text_encoder(captions, device).type(torch.float32)\n","                    encoded_captions_1 = text_encoder(captions_1, device).type(torch.float32)\n","                    encoded_captions_2 = text_encoder(captions_2, device).type(torch.float32)\n","\n","                    ###### Discriminator ######\n","                    # real\n","                    label = torch.full((b_size,), 1, dtype=torch.float, device=device)\n","                    real_D = model_D(real_lmks, encoded_captions).view(-1)\n","                    loss_real_D = criterion(real_D, label)\n","\n","                    # fake\n","                    z = torch.normal(0, 1, size=(b_size, 128)).to(device)\n","                    fake_lmks = model_G(z, encoded_captions).detach()\n","                    label = torch.full((b_size,), 0, dtype=torch.float, device=device)\n","                    fake_D = model_D(fake_lmks, encoded_captions).view(-1)\n","                    loss_fake_D = criterion(fake_D, label)\n","\n","                    # grad penalty\n","                    # alpha = torch.rand(b_size, 1, 1, 1).to(device)\n","                    # interp_lmks = alpha * real_lmks.data + (1- alpha) * fake_lmks.data\n","                    # interp_lmks = Variable(interp_lmks, requires_grad=True).to(device)\n","                    # interp_D = model_D(interp_lmks, encoded_captions)\n","                    # weight = torch.ones(interp_D.size()).to(device)\n","                    # gradients = torch.autograd.grad(outputs=interp_D,\n","                    #                                 inputs=interp_lmks,\n","                    #                                 grad_outputs=weight,\n","                    #                                 retain_graph=True,\n","                    #                                 create_graph=True,\n","                    #                                 only_inputs=True)[0]\n","                    # gradients = gradients.view(gradients.size(0), -1)\n","                    # gradients_norm = torch.sqrt(torch.sum(gradients ** 2, dim=1) + 1e-12)\n","                    # loss_gp_D = ((gradients_norm - 1) ** 2).mean()\n","\n","                    # discriminator loss\n","                    loss_D = loss_real_D + loss_fake_D\n","                    avg_loss_D_valid += loss_D.item()\n","\n","                    ###### Generator ######\n","                    z = torch.normal(0, 1, size=(b_size, 128)).to(device)\n","                    fake_lmks = model_G(z, encoded_captions)\n","                    fake_D = model_D(fake_lmks, encoded_captions).view(-1)\n","                    label = torch.full((b_size,), 1, dtype=torch.float, device=device)\n","                    loss_fake_G = criterion(fake_D, label)\n","\n","                    interp_captions = (encoded_captions_1 + encoded_captions_2) / 2\n","                    z = torch.normal(0, 1, size=(b_size, 128)).to(device)\n","                    fake_interp_lmks = model_G(z, interp_captions)\n","                    fake_interp_D = model_D(fake_interp_lmks, interp_captions).view(-1)\n","                    loss_interp_G = criterion(fake_interp_D, label)\n","\n","                    loss_G = loss_fake_G + loss_interp_G\n","                    # loss_G = (loss_fake_G + loss_interp_G) / 2\n","\n","                    avg_loss_G_valid += loss_G.item()\n","\n","                    it.set_postfix(\n","                        ordered_dict={\n","                            \"loss_D\": avg_loss_G_valid / batch_no,\n","                            \"loss_G\": avg_loss_D_valid / batch_no,\n","                            \"epoch\": epoch_no,\n","                        },\n","                        refresh=False,\n","                    )\n","\n","        valid_loss_G.append(avg_loss_G_valid / batch_no)\n","        valid_loss_D.append(avg_loss_D_valid / batch_no)\n","        valid_loss_epoch.append(epoch_no)\n","        if best_valid_loss > (avg_loss_G_valid + avg_loss_D_valid) / 2:\n","            best_valid_loss = (avg_loss_G_valid + avg_loss_D_valid) / 2\n","            print(f\"\\n Best G loss is updated to {avg_loss_G_valid / batch_no} at {epoch_no}\")\n","            print(f\"Best D loss is updated to {avg_loss_D_valid / batch_no} at {epoch_no}\")\n","            save_state(model_G, model_D, optimizer_G, optimizer_D, epoch_no, best=True)\n","\n","    if (epoch_no + 1) == n_epoch:\n","        plot_loss(train_loss_epoch, valid_loss_epoch, train_loss_G, valid_loss_G, name='G')\n","        plot_loss(train_loss_epoch, valid_loss_epoch, train_loss_D, valid_loss_D, name='D')\n","\n","    save_state(model_G, model_D, optimizer_G, optimizer_D, epoch_no)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def vis(real_lmks):\n","    fig = plt.figure(figsize=(16, 16))\n","\n","    for i in range(16):\n","        kB = heatmaps2keypoints(real_lmks[i], 0.2)\n","        real_lmk = np.where(kB < 0, -1, kB * 4)\n","\n","        ax = fig.add_subplot(8, 8, i + 1)\n","        ax.title.set_text('Real')\n","        ax.scatter(real_lmk[0, 0], real_lmk[0, 1])\n","        ax.axis('off')\n","\n","    plt.show()\n","\n","for batch in train_loader:\n","    landmarks, captions, captions_1, captions_2 = batch\n","    landmarks = landmarks.type(torch.float)\n","    print(len(landmarks), len(train_loader))\n","#     vis(landmarks[:64].detach().cpu().numpy())"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
